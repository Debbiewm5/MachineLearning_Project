
---
title: "Machine Learning_Project"
author: "Dr. D. Weissman-Miller"
date: "March 16, 2015"
output: html_document
---

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

The goal of this project is to predict the manner of performing unilateral dumbbell biceps curls based on data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants includeing:

A: exactly according to the specification
B: throwing the elbows to the front
C: lifting the dumbbell only halfway
D: lowering the dumbbell only halfway
E: throwing the hips to the front

## Building the Model
The first step is to prepare the datasets and remove the first ID variable so that it does not interfere with the ML algorithims, then to examine the data, and then to remove excess NA's.

### Setup working directory and prepare the datasets

```{r,echo=TRUE}
setwd("~/Documents/MachineLearning_ Project")
library(caret)
library(lattice)
library(ggplot2)
library(plyr)
library(sandwich)

train <- read.csv("~/Documents/MachineLearning_ Project/pml-training.csv")
test <- read.csv("~/Documents/MachineLearning_ Project/pml-testing.csv")

train <- train[,-1]
test <- test[,-1]
```
### Examine the train dataset

```{r,echo=TRUE, results='hide'}
head(train, n=10)
names(train)
```
### remove columns with too many NA's which are useless or empty variables for this prediction.

```{r,echo=TRUE}
col2remove <- apply(!is.na(train), 2, sum)>5000
train <- train[,col2remove]
test <- test[,col2remove]

train$user_name <- factor(train$user_name)
train$cvtd_timestamp <-  factor(train$cvtd_timestamp)
train$new_window <- factor(train$new_window)
train$classe <- factor(train$classe)

test$user_name <- factor(test$user_name)
test$cvtd_timestamp <-  factor(test$cvtd_timestamp)
test$new_window <- factor(test$new_window)
```
### create numeric data columns

```{r,echo=TRUE}
numeric_cols <- sapply(train, is.numeric)
```

### preprocessing for training data

```{r,echo=TRUE}
prep <- preProcess(train[, numeric_cols], method = c("center","scale","medianImpute"))
newTrain <- predict(prep, train[, numeric_cols])
newTrain <- cbind(user_name=train$user_name,cvtd_timestamp=train$cvtd_timestamp,new_window=train$new_window,newTrain)
```

### preprocessing for testing data

```{r,echo=TRUE}
newTest <- predict(prep, test[, numeric_cols])
newTest <- cbind(user_name=test$user_name,cvtd_timestamp=test$cvtd_timestamp,new_window=test$new_window,newTest)
```

## Training the model
### The inital model 'training' is first trained using a generalized boosted model (with trees) 'gbm' to examine the machine learning possibilities with this dataset and to predict on an initial subsample of the 'train' dataset. Boosting is selected because of the somewhat sparse data which would be augmented by building the model in a stage-wise fashion. The second step uses a reduced model set 'newTrain' where cross-validation is used to select the best final model.  

```{r,echo=TRUE}
set.seed(1203458)
inTrain = createDataPartition(train$classe, p = 0.9, list=FALSE)
training = train[inTrain,]
testing = train[-inTrain,]
dim(training); dim(testing)

## The following code has been fitted due to lack of variance in variables such as kurtosis_yaw_dumbell, where there were 50 warnings showing no variation.

modFit <- train(classe ~ user_name + pitch_arm + yaw_arm + roll_arm + roll_belt + pitch_belt + yaw_belt + gyros_belt_x + gyros_belt_y +  gyros_belt_z + accel_belt_x + accel_belt_y + accel_belt_z + magnet_belt_x + magnet_belt_y + magnet_belt_z + gyros_arm_x + gyros_arm_y + gyros_arm_z + accel_arm_x + accel_arm_y + accel_arm_z + magnet_arm_x + magnet_arm_y + magnet_arm_z + roll_dumbbell + pitch_dumbbell + yaw_dumbbell, method="gbm", data=training, verbose=FALSE)

print(modFit)
predtrain <- predict(modFit,training)
table(predtrain, training$classe)
```
### The training model captures 91.4% of observations in the p=0.9 training sample using 150 trees. The data shows that "roll_belt" and "yaw_belt" are the most important features.

```{r,echo=TRUE}
summary(modFit, n.trees=150)
```
### Gradient boosting efficacy plot

```{r, echo=TRUE}
ggplot(modFit)
##trellis.par.set(caretTheme())
```
### Performance on the p= p=0.1 sample reserved for testing, where the results show a nearly identical result (approximately 0.46% lower) performance compared to the training set.

```{r,echo=TRUE}
predTest <- predict(modFit, testing)
table(predTest, testing$classe)
```

### Cross-validation is used to select the best model
The package 'sandwich' is then installed.

```{r,echo=TRUE}
set.seed(1203458)
ctrl <- trainControl(method = "cv")
fit <- train(train[,]$classe~., method="ctree",data=newTrain[,], trControl = ctrl)
```
## Model Evaluation
The accuracy for cross-validation results are given below:
        
```{r,echo=TRUE}
fit$results
```
### The best accuracy is given by the model with the minicriterion 0.01.  This model has the highest accuracy of 0.9803799, with a Kappa of 0.9751840. From these analyses, it can be seen that the cross-validation model has better accuracy than the Stochastic Gradient Boosting algorithm using the 'gbm' package.

## Making Predictions
The code for the  predictions is given below:

```{r,echo=TRUE}
prediction <- predict(fit, newTest)
```
### The predictions are as follws:

```{r,echo=TRUE}
prediction
```
### Algorithm to predict test results
```{r,echo=TRUE}

pml.testing <- read.csv("~/Documents/MachineLearning_ Project/pml-testing.csv")
answers <- as.character(predict(modFit, pml.testing))
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)
```




